{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f02bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FL reference: https://towardsdatascience.com/preserving-data-privacy-in-deep-learning-part-1-a04894f78029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90b34b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataset import Dataset   \n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d28228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install medmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d1038ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea75752",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Hyperparameters for federated learning #########\n",
    "num_clients = 3\n",
    "num_selected = 3\n",
    "num_rounds = 2\n",
    "epochs = 3\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a26953d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedMNIST v2.1.0 @ https://github.com/MedMNIST/MedMNIST/\n"
     ]
    }
   ],
   "source": [
    "print(f\"MedMNIST v{medmnist.__version__} @ {medmnist.HOMEPAGE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d9464e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag = 'chestmnist'\n",
    "download = True\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 128\n",
    "lr = 0.001\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12f28f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\ibrah\\.medmnist\\chestmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\ibrah\\.medmnist\\chestmnist.npz\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "\n",
    "traindata_split = torch.utils.data.random_split(train_dataset, [int(train_dataset.imgs.shape[0] / num_clients) for _ in range(num_clients)])\n",
    "\n",
    "train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "\n",
    "#test_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in testdata_split]\n",
    "\n",
    "\n",
    "\n",
    "# # Dividing the training data into num_clients, with each client having equal number of images\n",
    "# traindata_split = torch.utils.data.random_split(train_dataset, [int(np.shape(train_dataset)[0] / num_clients) for _ in range(num_clients)])\n",
    "# # Creating a pytorch loader for a Deep Learning model\n",
    "# train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "# # Loading the test iamges and thus converting them into a test_loader\n",
    "# \n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# pil_dataset = DataClass(split='train', download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "#train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "#test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "294b2491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22433"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.imgs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efa28edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ChestMNIST (chestmnist)\n",
      "    Number of datapoints: 78468\n",
      "    Root location: C:\\Users\\ibrah\\.medmnist\n",
      "    Split: train\n",
      "    Task: multi-label, binary-class\n",
      "    Number of channels: 1\n",
      "    Meaning of labels: {'0': 'atelectasis', '1': 'cardiomegaly', '2': 'effusion', '3': 'infiltration', '4': 'mass', '5': 'nodule', '6': 'pneumonia', '7': 'pneumothorax', '8': 'consolidation', '9': 'edema', '10': 'emphysema', '11': 'fibrosis', '12': 'pleural', '13': 'hernia'}\n",
      "    Number of samples: {'train': 78468, 'val': 11219, 'test': 22433}\n",
      "    Description: The ChestMNIST is based on the NIH-ChestXray14 dataset, a dataset comprising 112,120 frontal-view X-Ray images of 30,805 unique patients with the text-mined 14 disease labels, which could be formulized as a multi-label binary-class classification task. We use the official data split, and resize the source images of 1×1024×1024 into 1×28×28.\n",
      "    License: CC BY 4.0\n",
      "===================\n",
      "Dataset ChestMNIST (chestmnist)\n",
      "    Number of datapoints: 22433\n",
      "    Root location: C:\\Users\\ibrah\\.medmnist\n",
      "    Split: test\n",
      "    Task: multi-label, binary-class\n",
      "    Number of channels: 1\n",
      "    Meaning of labels: {'0': 'atelectasis', '1': 'cardiomegaly', '2': 'effusion', '3': 'infiltration', '4': 'mass', '5': 'nodule', '6': 'pneumonia', '7': 'pneumothorax', '8': 'consolidation', '9': 'edema', '10': 'emphysema', '11': 'fibrosis', '12': 'pleural', '13': 'hernia'}\n",
      "    Number of samples: {'train': 78468, 'val': 11219, 'test': 22433}\n",
      "    Description: The ChestMNIST is based on the NIH-ChestXray14 dataset, a dataset comprising 112,120 frontal-view X-Ray images of 30,805 unique patients with the text-mined 14 disease labels, which could be formulized as a multi-label binary-class classification task. We use the official data split, and resize the source images of 1×1024×1024 into 1×28×28.\n",
      "    License: CC BY 4.0\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(\"===================\")\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b1a2d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\medmnist\\utils.py:25: FutureWarning: `multichannel` is a deprecated argument name for `montage`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  montage_arr = skimage_montage(sel_img, multichannel=(n_channels == 3))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACWklEQVR4nAXB227bRhAA0JnZ2V1SJFcSpcSWDdtAghToSwsUKNCv7h/0N/rSvCRA7MR2DEoiRe51eg7eLkWo7vuNu//909ZgOr88ff7xmtw/nrgUavv9urWGS8gAIETNrk7m5nFhcA93ewcZ7LqWaKAU1ey0h+rPp6/412+/rtBgBNJVfVgLDY9H5JyUn175j483HaG3anz+potCe/xyat8fcI79Ld/tt2uOZ1JhHJrj0dBlWuy4a7QuQjfvGssF6eWNbfSTQCm1mR4D2/Wat61Vhax/eTSGK2GATT4H2XeghJw1jGyPw2nwAlazgdNznAckUlwbRGR/9qei6sBGTuPrqDHkCpkJkgIVp4XH775dmayOc+tKSIzCJegCMPvJp0iTUlIt4e4eYgQpFDIIwfQ2LdjtrjUqd2XNwyedBYQSikAaplQi1muNWDm3kI2TiDCVoFQYTmdVdzvHiKZrvv3dU5VVYYq6wPyGTVld940m1E1bvRw3hyKB8qUgTrrr2FYBgYgrb64aCKlkfk4ryF6jVdcgTCTa3cST7djbxP/eJfGXUqd6k3c1gmL34BcxECXyf9sgJ6+3S5i0YaUAdf8hRliWnOjnKaQ3wGnOoiwRESm7OyjiMUUeo4Q5xKNvdVMjIoBC3V7AjIvwOcg4DGrTOOcqQgSRAobnwV+ApwUu5+7j3iIiKwJASH5ZnX+6acXLpcgvtxtNSACGCEAE26vx1c81xzH37xwjIQEwISjCVdcf3j/FzHksvdOMSAioCIS0iLZ21V4Cy5gNEyEhIqAIEOcIDO39j+l/Y3Y+iR+ld3QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualization\n",
    "train_dataset.montage(length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79d35d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple CNN model\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = Net(in_channels=n_channels, num_classes=n_classes)\n",
    "    \n",
    "# define loss function and optimizer\n",
    "if task == \"multi-label, binary-class\":\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58e0f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_update(client_model, optimizer, train_loader, epoch=5):\n",
    "    \"\"\"\n",
    "    This function updates/trains client model on client data\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for e in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.cuda(), target.cuda().float()\n",
    "            optimizer.zero_grad()\n",
    "            output = client_model(data).float()\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c38793ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def server_aggregate(global_model, client_models):\n",
    "    \"\"\"\n",
    "    This function has aggregation method 'mean'\n",
    "    \"\"\"\n",
    "    ### This will take simple mean of the weights of models ###\n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        global_dict[k] = torch.stack([client_models[i].state_dict()[k].float() for i in range(len(client_models))], 0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(global_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab0c8812",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = info['task']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "518eee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medmnist import INFO, Evaluator\n",
    "def test(global_model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = torch.tensor([]).cuda().float()\n",
    "    y_score = torch.tensor([]).cuda().float()\n",
    "    \n",
    "    data_loader = test_loader\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            outputs = global_model(inputs.cuda().float())\n",
    "\n",
    "            if task == 'multi-label, binary-class':\n",
    "                targets = targets.cuda().float()\n",
    "                outputs = outputs.softmax(dim=-1).cuda().float()\n",
    "            else:\n",
    "                targets = targets.squeeze().cuda().float()\n",
    "                outputs = outputs.softmax(dim=-1).cuda().float()\n",
    "                targets = targets.float().resize_(len(targets), 1).cuda().float()\n",
    "\n",
    "            y_true = torch.cat((y_true, targets), 0)\n",
    "            y_score = torch.cat((y_score, outputs), 0)\n",
    "\n",
    "        y_true = y_true.cpu().numpy()\n",
    "\n",
    "        y_score = y_score.cpu().numpy()\n",
    "        \n",
    "        evaluator = Evaluator(data_flag, 'test')\n",
    "        metrics = evaluator.evaluate(y_score)\n",
    "    \n",
    "        print('%s  auc: %.3f  acc:%.3f' % ('test', *metrics))\n",
    "        return metrics.ACC, metrics.AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7221face",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################\n",
    "#### Initializing models and optimizer  ####\n",
    "############################################\n",
    "\n",
    "#### global model ##########\n",
    "global_model =  Net(in_channels=n_channels, num_classes=n_classes).cuda()\n",
    "\n",
    "############## client models ##############\n",
    "client_models = [ Net(in_channels=n_channels, num_classes=n_classes).cuda() for _ in range(num_selected)]\n",
    "for model in client_models:\n",
    "    model.load_state_dict(global_model.state_dict()) ### initial synchronizing with global model \n",
    "\n",
    "############### optimizers ################\n",
    "opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15b79c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     train_correct = 0\n",
    "#     train_total = 0\n",
    "#     test_correct = 0\n",
    "#     test_total = 0\n",
    "    \n",
    "#     model.train()\n",
    "#     for inputs, targets in tqdm(train_loader):\n",
    "#         # forward + backward + optimize\n",
    "#         inputs, targets = inputs.cuda(), targets.cuda() # add this line\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "        \n",
    "#         if task == 'multi-label, binary-class':\n",
    "#             targets = targets.to(torch.float32)\n",
    "#             loss = criterion(outputs, targets)\n",
    "#         else:\n",
    "#             targets = targets.squeeze().long()\n",
    "#             loss = criterion(outputs, targets)\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72f9bae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:23<00:00, 27.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  auc: 0.504  acc:0.947\n",
      "0-th round\n",
      "average train loss 1.7 | auc test 0.504 | test acc: 0.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:11<00:00, 23.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  auc: 0.497  acc:0.947\n",
      "1-th round\n",
      "average train loss 1.35 | auc test 0.497 | test acc: 0.947\n"
     ]
    }
   ],
   "source": [
    "###### List containing info about learning #########\n",
    "losses_train = []\n",
    "auc_test = []\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "# Runnining FL\n",
    "\n",
    "for r in range(num_rounds):\n",
    "    # select random clients\n",
    "    client_idx = np.random.permutation(num_clients)[:num_selected]\n",
    "    # client update\n",
    "    loss = 0\n",
    "    for i in tqdm(range(num_selected)):\n",
    "        loss += client_update(client_models[i], opt[i], train_loader[client_idx[i]], epoch=epochs)\n",
    "    \n",
    "    losses_train.append(loss)\n",
    "    # server aggregate\n",
    "    server_aggregate(global_model, client_models)\n",
    "    \n",
    "    acc, auc = test(global_model, test_loader)\n",
    "    auc_test.append(auc)\n",
    "    acc_test.append(acc)\n",
    "    print('%d-th round' % r)\n",
    "    print('average train loss %0.3g | auc test %0.3g | test acc: %0.3f' % (loss / num_selected, auc, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff93d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.device(0)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719fb518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
